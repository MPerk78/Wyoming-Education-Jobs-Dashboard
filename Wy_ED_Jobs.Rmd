---
title: "R Job Scrape"
author: "Mark Perkins"
date: "2025-09-18"
output: html_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# This Markdown file contains all the code to scrape data from Wyoming's k12 and higher education job postings

## Part 1: Wyoming K12 Data

### Loop K12 data pulled from all districts that use applitrack
```{r loop with district sites on applitrack, message=FALSE, warning=FALSE, echo=TRUE}
# Load required libraries
library(rvest)
library(RSelenium)
library(dplyr)
library(readr)

# Read the CSV file
csv_file <- "Frontline_Job_Links.csv"
job_links <- read_csv(csv_file)

# Start RSelenium with Firefox browser
rD <- suppressMessages(rsDriver(browser = "firefox", port = 4444L, check = FALSE, chromever = NULL))
remDr <- rD$client

# Initialize list to store job data from all URLs
all_jobs_data <- list()

# Iterate over each row in the CSV file
for (i in 1:nrow(job_links)) {
  url <- job_links$JobSite[i]
  school_district <- job_links$`School District`[i]  # Extract the School District
  
  # Load the page
  remDr$navigate(url)
  
  # Wait for dynamic content to load (adjust time as needed)
  Sys.sleep(10)  # Adjust as needed based on page loading time
  
  # Get page source and parse with rvest
  page_source <- remDr$getPageSource()[[1]]
  soup <- read_html(page_source)
  
  # Extract job listings
  list_items <- soup %>%
    html_nodes("ul.postingsList")
  
  # Debug: print the number of job listings found
  cat("Number of job listings found at", url, ":", length(list_items), "\n")
  
  # Initialize list to store job data for the current URL
  jobs_data <- list()
  
  if (length(list_items) == 0) {
    # If no jobs found, create a placeholder row
    jobs_data <- list(list(
      title = "No jobs",
      position = NA,
      position2 = NA,
      date_posted = NA,
      location = NA,
      closing_date = NA,
      school_district = school_district
    ))
  } else {
    # Iterate over each job listing
    for (item in list_items) {
      current_job <- list()
      
      # Extract job title
      current_job$title <- item %>%
        html_node("table.title td#wrapword") %>%
        html_text(trim = TRUE)
      
      # Extract additional details
      labels <- item %>%
        html_nodes("li span.label") %>%
        html_text(trim = TRUE)
      
      values <- item %>%
        html_nodes("li span.normal") %>%
        html_text(trim = TRUE)
      
      # Match labels and values
      j <- 1
      for (k in seq_along(labels)) {
        label <- labels[k]
        if (label == "Position Type:") {
          current_job$position <- values[j]
          if (j + 1 <= length(values) && !grepl(":", values[j + 1])) {
            current_job$position2 <- values[j + 1]
            j <- j + 1
          } else {
            current_job$position2 <- NULL
          }
        } else if (label == "Date Posted:") {
          current_job$date_posted <- values[j]
        } else if (label == "Location:") {
          current_job$location <- values[j]
        } else if (label == "Closing Date:") {
          current_job$closing_date <- values[j]
        }
        j <- j + 1
      }
      
      # Add the School District information
      current_job$school_district <- school_district
      
      # Append current job data
      jobs_data <- c(jobs_data, list(current_job))
    }
  }
  
  # Append jobs data for this district to overall results
  all_jobs_data <- c(all_jobs_data, jobs_data)
}

# Close the RSelenium session
remDr$close()
rD$server$stop()

# Convert list of all job data to data frame
all_jobs_df <- bind_rows(all_jobs_data) %>%
  mutate(
    new_position = if_else(grepl("^\\d", position2), position, position),
    new_position2 = if_else(grepl("^\\d", position2), NA_character_, position2),
    new_date_posted = if_else(grepl("^\\d", position2), position2, date_posted),
    new_location = if_else(grepl("^\\d", position2), date_posted, location),
    new_closing_date = if_else(grepl("^\\d", position2), location, closing_date)
  ) %>%
  select(
    title, 
    new_position, 
    new_position2, 
    new_date_posted, 
    new_location, 
    new_closing_date, 
    school_district
  ) %>%
  rename(
    position = new_position,
    position2 = new_position2,
    date_posted = new_date_posted,
    location = new_location,
    closing_date = new_closing_date
  )

# Join with original job_links for extra metadata
all_jobs_df <- left_join(all_jobs_df, job_links, by = c("school_district" = "School District"))

# Save data frame to CSV file
write.csv(all_jobs_df, "all_job_listings.csv", row.names = FALSE)

```

### Loop k12 data from districts that use Tedd k12
```{r tedk12, message=FALSE, warning=FALSE, echo=TRUE}

# Load required libraries
library(rvest)
library(RSelenium)
library(dplyr)
library(readr)
library(purrr)

# Read the CSV file containing the URLs
tedjoblinks <- read.csv("tedk12_job_links.csv", stringsAsFactors = FALSE)

# Initialize an empty data frame to store the job data
all_ted_data <- data.frame(
  title = character(),
  date_posted = character(),
  position = character(),
  location = character(),
  url = character(),
  stringsAsFactors = FALSE
)

# Loop through each URL in the CSV file
for (i in 1:nrow(tedjoblinks)) {
  url <- tedjoblinks$Job.Link[i]  # Using the Job.Link column for URLs
  
  # Read the HTML content of the page
  page <- read_html(url)
  
  # Extract job title, posting date, type, and location
  jobs <- page %>%
    html_nodes("tr") %>% # Assuming each job is within a <tr> element
    map_df(~{
      title <- .x %>%
        html_node("td:nth-child(1) a") %>%
        html_text(trim = TRUE)
      
      date_posted <- .x %>%
        html_node("td:nth-child(2)") %>%
        html_text(trim = TRUE)
      
      position <- .x %>%
        html_node("td:nth-child(3)") %>%
        html_text(trim = TRUE)
      
      location <- .x %>%
        html_node("td:nth-child(4)") %>%
        html_text(trim = TRUE)
      
      data.frame(
        title = title,
        date_posted = date_posted,
        position = position,
        location = location,
        url = url,  # Add the URL to the data frame
        stringsAsFactors = FALSE
      )
    })
  
  # Append the jobs data to the all_jobs_data data frame
  all_ted_data <- bind_rows(all_ted_data, jobs)
}

# Remove rows with NA or "Job Title" in the job_title column
all_ted_data <- all_ted_data %>%
  filter(!is.na(title) & title != "Job Title")

all_ted_data<-left_join(all_ted_data, tedjoblinks, by = c("url" = "Job.Link"))


# Save the extracted data to a CSV file
write.csv(all_ted_data, "all_tedk12_job_listings.csv", row.names = FALSE)

```

### Loop k12 data from all districts that use springer school
```{r jobs from spring school, message=FALSE, warning=FALSE, echo=TRUE}

# Load required libraries
library(RSelenium)
library(rvest)
library(dplyr)
library(stringr)
library(lubridate)
library(purrr)

# ----------------------------
# Start RSelenium
# ----------------------------
rD <- rsDriver(browser = "firefox", port = 4444L, check = FALSE, chromever = NULL)
remDr <- rD$client

# ----------------------------
# Read the CSV file containing the URLs
# ----------------------------
springerjoblinks <- read.csv("springer_job_links.csv", stringsAsFactors = FALSE)

# ----------------------------
# Initialize empty data frame
# ----------------------------
all_springer_data <- data.frame(
  title = character(),
  date_posted_text = character(),
  date_posted = as.Date(character()),
  position = character(),
  location = character(),
  url = character(),
  district = character(),
  stringsAsFactors = FALSE
)

# ----------------------------
# Helper functions
# ----------------------------

# Wait for an element to appear
wait_for_element <- function(remDr, css_selector, timeout = 10) {
  for (i in 1:timeout) {
    element <- tryCatch(remDr$findElement(using = "css selector", css_selector), error = function(e) NULL)
    if (!is.null(element)) return(element)
    Sys.sleep(1)
  }
  return(NULL)
}

# Safely extract text
safe_html_text <- function(node) {
  tryCatch(html_text(node, trim = TRUE), error = function(e) NA_character_)
}

# Convert relative dates to actual dates
resolve_relative_date <- function(text, reference_date = Sys.Date()) {
  text <- str_trim(text)
  case_when(
    str_detect(text, regex("^today$", ignore_case = TRUE)) ~ reference_date,
    str_detect(text, regex("^yesterday$", ignore_case = TRUE)) ~ reference_date - days(1),
    str_detect(text, regex("\\d+\\s+days?\\s+ago", ignore_case = TRUE)) ~
      reference_date - days(as.numeric(str_extract(text, "\\d+"))),
    TRUE ~ suppressWarnings(as.Date(str_extract(text, "\\w{3} \\d{2}, \\d{4}"), format = "%b %d, %Y"))
  )
}

# ----------------------------
# Scrape a single URL
# ----------------------------
scrape_jobs <- function(remDr, url) {
  remDr$navigate(url)
  Sys.sleep(3)  # initial load

  # Close pop-up if it exists
  tryCatch({
    close_btn <- remDr$findElement(using = "css selector", "#navigate-away-from-section-dialog-btn-cancel")
    close_btn$clickElement()
  }, error = function(e) NULL)

  # Scroll and click "More Jobs" until fully loaded
  max_attempts <- 30
  attempts <- 0
  last_count <- 0
  buffer_clicks <- 3

  repeat {
    remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")
    Sys.sleep(1.5)

    page <- read_html(remDr$getPageSource()[[1]])
    jobs_nodes <- html_nodes(page, xpath = "//div[contains(@class, 'card-body')]")
    current_count <- length(jobs_nodes)

    if (current_count == last_count) {
      if (buffer_clicks > 0) {
        buffer_clicks <- buffer_clicks - 1
      } else break
    } else {
      last_count <- current_count
      buffer_clicks <- 3
    }

    # Click "More Jobs" if present
    more_btn <- tryCatch(wait_for_element(remDr, ".pds-secondary-btn"), error = function(e) NULL)
    if (!is.null(more_btn)) {
      more_btn$clickElement()
      Sys.sleep(2)
      attempts <- attempts + 1
    } else break

    if (attempts >= max_attempts) break
  }

  # Extract job info
  months_vec <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")
  jobs_data <- map_df(jobs_nodes, ~{
    title <- safe_html_text(html_node(.x, xpath = ".//div[contains(@class, 'card-title')]/div"))
    paragraphs <- html_nodes(.x, "p") %>% map_chr(safe_html_text)

    # Extract position and location by order of <p> elements
    position <- if(length(paragraphs) >= 1) paragraphs[1] else NA_character_
    location <- if(length(paragraphs) >= 2) paragraphs[2] else NA_character_
    date_posted_text <- if(length(paragraphs) >= 3) paragraphs[3] else NA_character_

    date_posted <- resolve_relative_date(date_posted_text)

    tibble(
      title = title,
      position = position,
      location = location,
      date_posted_text = date_posted_text,
      date_posted = date_posted,
      url = url,
      district = NA_character_
    )
  })

  return(jobs_data)
}


# ----------------------------
# Loop through each URL and scrape
# ----------------------------
for (i in 1:nrow(springerjoblinks)) {
  url <- springerjoblinks$`Job.Link`[i]
  message(paste("Processing URL:", url))
  
  job_data <- scrape_jobs(remDr, url)
  
  if (nrow(job_data) > 0) {
    all_springer_data <- bind_rows(all_springer_data, job_data)
    message(paste("Scraped", nrow(job_data), "jobs from:", url))
  } else {
    message("No jobs found at:", url)
  }
}

# ----------------------------
# Merge with original CSV and clean up
# ----------------------------
all_springer_data <- left_join(all_springer_data, springerjoblinks, by = c("url" = "Job.Link"))

all_springer_data <- all_springer_data %>%
  mutate(date_posted = resolve_relative_date(date_posted_text)) %>%
  dplyr::select(-c(district, date_posted_text))

# ----------------------------
# Save to CSV
# ----------------------------
write.csv(all_springer_data, "all_springer_job_listings.csv", row.names = FALSE)

# ----------------------------
# Close RSelenium
# ----------------------------
remDr$close()
rD$server$stop()



```

### Combine all the k12 data include a few manual pulls from unique district sites
#### Be sure to update your manually scraped data
#### This will populate your app folder for your Shiny application
```{r combine all the data into one dataframe, echo=TRUE, warning=FALSE, message=FALSE}
library(stringr)
library(tidyverse)
library(dplyr)
library(lubridate)


standardize_date <- function(x) {

  # Convert factor to character safely
  x <- as.character(x)

  # Remove timezone text like "(UTC)"
  x <- str_remove(x, "\\s*\\(.*\\)$")

  # Parse known formats
  parsed <- parse_date_time(
    x,
    orders = c(
      "b d Y h:M p",   # Oct 23 2025 6:00 AM
      "B d Y h:M p",   # October 23 2025 6:00 AM
      "ymd",
      "mdy",
      "dmy",
      "Y-m-d H:M:S"
    ),
    tz = "UTC",
    quiet = TRUE
  )

  as.Date(parsed)
}



frontline<- read.csv("all_job_listings.csv")%>%
  rename(url = JobSite)

frontline<-  dplyr::select(frontline, title, date_posted, position, location, url, school_district)%>%
  rename(District = school_district)

ted<- read.csv("all_tedk12_job_listings.csv")

springer<- read.csv("all_springer_job_listings.csv")

other<- readxl::read_excel("otherjobs.xlsx")

frontline <- frontline %>%
  mutate(date_posted = standardize_date(date_posted))

ted <- ted %>%
  mutate(date_posted = standardize_date(date_posted))

springer <- springer %>%
  mutate(date_posted = standardize_date(date_posted))

other <- other %>%
  mutate(date_posted = standardize_date(date_posted))


combined<- rbind(frontline, ted, springer, other)%>%
  mutate(Archive_Date = Sys.Date())



file_name <- paste0("combined_", Sys.Date(), ".csv")

# Write the data to a CSV file with the new file name
write.csv(combined, file=file.path('Archivek12_data',file_name))

# Code current data to categorize positions in the "position" column of the "combinedclean" dataset

combinedclean <- combined %>%
  mutate(
    position = case_when(
       # Paraprofessional-related positions
      grepl("Paraprofessional|Para|Paraeducator", title, ignore.case = TRUE) ~ "Paraprofessional",

     # Substitute
      grepl("Sub|Substitute", title, ignore.case = TRUE) ~ "Substitute",
     
      # Support Services-related positions
      grepl("Counselor|Psychologist|Therapist|Pathologist|Occupational|Alternative Behavior Program|Audiologist|Lifeguard|Interpreter|Translator|Monitor|Safety Patrol|Social Worker|Technology Intern|Tutor|Instructional Facilitator|Resource Officer|Behavior|Occupational|Aide|Aid|Support", 
            title, ignore.case = TRUE) ~ "Support Services",
      
      # Teacher-related positions
      grepl("Teacher|Instructor|Interventionist|English|Math|Library|Media|Language Arts|Science|Educator", 
            title, ignore.case = TRUE) ~ "Teacher",
      
      # Coach-related positions
      grepl("Coach|Athletic|Sports|Track|Football|Basketball|Golf|Soccer|Wrestling|Cheer|BB|Volleyball|Aquatics|Ski", 
            title, ignore.case = TRUE) ~ "Athletics",

      # Custodial/Maintenance-related positions
      grepl("Custodian|Custodial|Maintenance|Cleaner|Snow Removal|Facility|Electrician|HVAC", 
            title, ignore.case = TRUE) ~ "Custodial/Maintenance",
      
      # Transportation-related positions
      grepl("Bus Driver|Driver|Transportation|Route|Bus|Mechanic|Bus Mechanic|Bus Monitor",
            title, ignore.case = TRUE) ~ "Transportation",

     
      # Administration-related positions
      grepl("Principal|Director|Coordinator|Administrator|Supervisor|Administrative", 
            title, ignore.case = TRUE) ~ "Administration",
      
      # Staff-related positions
      grepl("Secretary|Office Clerk|Office Manager|Desk Clerk|Aide", title, ignore.case = TRUE) ~ "Staff",

      # Food Services-related positions
      grepl("Food|Cook|Nutrition|Cafeteria|Dishwasher", title, ignore.case = TRUE) ~ "Food Services",

      # Student Teaching-related positions
      grepl("Daycare|After School|Head Start", title, ignore.case = TRUE) ~ "Child Care",
      
      # Other positions
      TRUE ~ "Other"
    )
  )%>%
  dplyr::select(title, date_posted, position, location, url, District)

write.csv(combinedclean, file=file.path('Wy_Ed_Jobs','combinedclean.csv'))


#Munge the archived data and send files to Shiny
csv_files <- list.files("Archivek12_Data", pattern = "\\.csv$", full.names = TRUE)

combined_k12_data <- csv_files %>%
  lapply(function(file) read.csv(file, colClasses = c("Archive_Date" = "character"))) %>%
  bind_rows() %>%
  select(-X, -date_posted)

combined_k12_data <- combined_k12_data %>%
  mutate(Archive_Date = case_when(
    grepl("/", Archive_Date) ~ as.Date(Archive_Date, format = "%m/%d/%Y"),
    grepl("-", Archive_Date) ~ as.Date(Archive_Date, format = "%Y-%m-%d"),
    TRUE ~ as.Date(NA)  # Corrected this line
  ))

dates <- as.data.frame(unique(combined_k12_data$Archive_Date))


combined_k12_data$Archive_Date <- as.Date(combined_k12_data$Archive_Date)

###################################################
# Create Categories

combinedclean <- combined_k12_data %>%
  mutate(
    position = case_when(
       # Paraprofessional-related positions
      grepl("Paraprofessional|Para|Paraeducator", title, ignore.case = TRUE) ~ "Paraprofessional",

     # Substitute
      grepl("Sub|Substitute", title, ignore.case = TRUE) ~ "Substitute",
     
      # Support Services-related positions
      grepl("Counselor|Psychologist|Therapist|Pathologist|Occupational|Alternative Behavior Program|Audiologist|Lifeguard|Interpreter|Translator|Monitor|Safety Patrol|Social Worker|Technology Intern|Tutor|Instructional Facilitator|Resource Officer|Behavior|Occupational|Aide|Aid|Support", 
            title, ignore.case = TRUE) ~ "Support Services",
      
      # Teacher-related positions
      grepl("Teacher|Instructor|Interventionist|English|Math|Library|Media|Language Arts|Science|Educator", 
            title, ignore.case = TRUE) ~ "Teacher",
      
      # Coach-related positions
      grepl("Coach|Athletic|Sports|Track|Football|Basketball|Golf|Soccer|Wrestling|Cheer|BB|Volleyball|Aquatics|Ski", 
            title, ignore.case = TRUE) ~ "Athletics",

      # Custodial/Maintenance-related positions
      grepl("Custodian|Custodial|Maintenance|Cleaner|Snow Removal|Facility|Electrician|HVAC", 
            title, ignore.case = TRUE) ~ "Custodial/Maintenance",
      
      # Transportation-related positions
      grepl("Bus Driver|Driver|Transportation|Route|Bus|Mechanic|Bus Mechanic|Bus Monitor",
            title, ignore.case = TRUE) ~ "Transportation",

     
      # Administration-related positions
      grepl("Principal|Director|Coordinator|Administrator|Supervisor|Administrative", 
            title, ignore.case = TRUE) ~ "Administration",
      
      # Staff-related positions
      grepl("Secretary|Office Clerk|Office Manager|Desk Clerk|Aide", title, ignore.case = TRUE) ~ "Staff",

      # Food Services-related positions
      grepl("Food|Cook|Nutrition|Cafeteria|Dishwasher", title, ignore.case = TRUE) ~ "Food Services",

      # Student Teaching-related positions
      grepl("Daycare|After School|Head Start", title, ignore.case = TRUE) ~ "Child Care",
      
      # Other positions
      TRUE ~ "Other"
    )
  )%>%
  dplyr::select(title, Archive_Date, position, location, url, District)

combinedclean <- combinedclean %>%
  mutate(District = case_when(
    District == "Hot Springs School District 1" ~ "Hot Springs County School District 1",
    District == "Crook County school District 1" ~ "Crook County School District 1",
    TRUE ~ District
  ))

k12jobs <- combinedclean %>%
  filter(position == "Teacher")

# Define the category keywords with enhanced regex
category_keywords <- list(
  "Technical Education" = "\\b(Welding|Weld|CTE|Automotive|Auto(?!nomy)|Wood|Woods|Industrial Arts|Shop)\\b",
  
  "Agriculture Education" = "\\b(Agriculture|Ag Teacher|FFA)\\b",
  
  "Computer Science Education" = "\\b(Computer Science|Information Technology|\\bIT\\b|Technology Education(?! Teacher Assistant))\\b",
  
  "Early Childhood Education" = "\\b(Early Childhood|Pre[- ]?K(?!indergarten)|Preschool|Birth to age|Ages 3-5)\\b",
  
  "Elementary Education" = "\\b(Elementary|Kindergarten|1st|2nd|3rd|4th|5th|First|Second|Third|Fourth|Fifth|Grade [1-6])\\b",
  
  "English Language Arts Education" = "(?i)\\b(English(?!\\s+as\\s+a\\s+Second\\s+Language|\\s+Learner|\\s+Language\\s+Learner)|ELA|Language Arts(?!\\s*Coordinator))\\b",

  "Family and Consumer Science" = "\\b(Family and Consumer Science|FCS|Home Economics)\\b",
  
  "Language Education" = "\\b(ESL|ELL|English as a Second Language|Bilingual|Spanish|French|Arapaho|Dual Language|World Language|Foreign Language)\\b",
  "Mathematics Education" = "\\b(Mathematics|\\bMath\\b)\\b",
  
  "Music Education" = "\\b(Music|Orchestra|Band)\\b",
  
  "Health Science" = "\\b(Nurse Educator|Health Science)\\b",
  
  "Science Education" = "\\b(Science|Biology|Chemistry|Physics|Earth Science)\\b",
  
  "Social Studies Education" = "\\b(Social Studies|History|Geography|Civics|Political Science)\\b",
  
  "Special Education" = "\\b(Resource Teacher|Special Education|SPED|Exceptional Children|Deaf|Visually Impaired)\\b",
  
  "Physical Education" = "(?i)\\b(Physical Education|P\\.?E\\.?|PE\\s*/\\s*Health|Health\\s*/\\s*PE|Health and Physical Education)\\b",
  
  "Business and Economics" = "\\b(Business|Economics|Econ)\\b",
  
  "Substitute Teaching" = "\\b(Substitute)\\b",
  
  "Virtual Education" = "\\b(Virtual|Online|Remote)\\b",
  
  "Art Teacher" = "\\b(Art Teacher|\\bArt\\b)\\b",
  
  "STEM Teacher" = "\\b(STEM)\\b",
  
  "CTE Teacher" = "\\b(Career and Technical Education|CTE Teacher)\\b"
)

k12jobs <- k12jobs %>%
  mutate(
    Category = map_chr(str_to_lower(title), function(title) {
      matched <- NULL
      for (category in names(category_keywords)) {
        pattern <- category_keywords[[category]]
        if (str_detect(title, regex(pattern, ignore_case = TRUE))) {
          matched <- category
          break
        }
      }
      if (is.null(matched)) "Uncategorized" else matched
    })
  )


# Mutate the Category variable based on title using the keywords
k12jobs <- k12jobs %>%
  mutate(Category = case_when(
    str_detect(title, regex(category_keywords[["Special Education"]], ignore_case = TRUE)) ~ "Special Education",  # Match Special Education first
    str_detect(title, regex(category_keywords[["Technical Education"]], ignore_case = TRUE)) ~ "Technical Education",
    str_detect(title, regex(category_keywords[["Agriculture Education"]], ignore_case = TRUE)) ~ "Agriculture Education",
    str_detect(title, regex(category_keywords[["Computer Science Education"]], ignore_case = TRUE)) ~ "Computer Science Education",
    str_detect(title, regex(category_keywords[["Early Childhood Education"]], ignore_case = TRUE)) ~ "Early Childhood Education",
    str_detect(title, regex(category_keywords[["Elementary Education"]], ignore_case = TRUE)) ~ "Elementary Education",
    str_detect(title, regex(category_keywords[["English Language Arts Education"]], ignore_case = TRUE)) ~ "English Language Arts Education",
    str_detect(title, regex(category_keywords[["Family and Consumer Science"]], ignore_case = TRUE)) ~ "Family and Consumer Science",
    str_detect(title, regex(category_keywords[["Language Education"]], ignore_case = TRUE)) ~ "Language Education",
    str_detect(title, regex(category_keywords[["Mathematics Education"]], ignore_case = TRUE)) ~ "Mathematics Education",
    str_detect(title, regex(category_keywords[["Music Education"]], ignore_case = TRUE)) ~ "Music Education",
    str_detect(title, regex(category_keywords[["Physical Education"]], ignore_case = TRUE)) ~ "Physical Education",  # Match Physical Education second
    str_detect(title, regex(category_keywords[["Health Science"]], ignore_case = TRUE)) ~ "Health Science",
    str_detect(title, regex(category_keywords[["Science Education"]], ignore_case = TRUE)) ~ "Science Education",
    str_detect(title, regex(category_keywords[["Social Studies Education"]], ignore_case = TRUE)) ~ "Social Studies Education",
    str_detect(title, regex(category_keywords[["Business and Economics"]], ignore_case = TRUE)) ~ "Business and Economics",
    str_detect(title, regex(category_keywords[["Substitute Teaching"]], ignore_case = TRUE)) ~ "Substitute Teaching",
    str_detect(title, regex(category_keywords[["Virtual Education"]], ignore_case = TRUE)) ~ "Virtual Education",
    str_detect(title, regex(category_keywords[["Art Teacher"]], ignore_case = TRUE)) ~ "Art Teacher",
    str_detect(title, regex(category_keywords[["STEM Teacher"]], ignore_case = TRUE)) ~ "STEM Teacher",
    str_detect(title, regex(category_keywords[["CTE Teacher"]], ignore_case = TRUE)) ~ "CTE Teacher",
    TRUE ~ "Uncategorized"  # Default case for uncategorized titles
  ))
k12jobs <- k12jobs %>%
  mutate(Broad_Category = case_when(
    Category == "Agriculture Education" ~ "CTE",
    Category == "Art Teacher" ~ "Art",
    Category == "Business and Economics" ~ "CTE",
    Category == "Computer Science Education" ~ "CTE",
    Category == "Early Childhood Education" ~ "Early Childhood",
    Category == "Elementary Education" ~ "Elementary",
    Category == "English Language Arts Education" ~ "English Language Arts Secondary",
    Category == "Family and Consumer Science" ~ "CTE",
    Category == "Health Science" ~ "CTE",
    Category == "Language Education" ~ "Lanugage",
    Category == "Mathematics Education" ~ "STEM",
    Category == "Music Education" ~ "Music",
    Category == "Physical Education" ~ "Physical Education",
    Category == "STEM Teacher" ~ "STEM",
    Category == "Science Education" ~ "STEM",
    Category == "Social Studies Education" ~ "Secondary Social Studies",
    Category == "Special Education" ~ "Special Education",
    Category == "Technical Education" ~ "CTE",
    Category == "Uncategorized" ~ "Other",
    Category == "Virtual Education" ~ "Other",
    TRUE ~ "Other"  # Default case to handle any unexpected categories
  ))

write.csv(k12jobs, file=file.path('Wy_Ed_Jobs','k12jobanalysis.csv'))


#Database for all dates by category 
k12sum <- k12jobs %>%
  group_by(Broad_Category, Archive_Date) %>%
  summarize(
    sum = n_distinct(title)
  ) %>%
  ungroup() %>%
  mutate(District = "Total") %>%
  select(Broad_Category, Archive_Date, District, sum)

k12sumdistrict <- k12jobs %>%
  group_by(Broad_Category, Archive_Date, District) %>%
  summarize(
    sum = n_distinct(title)
  ) %>%
  ungroup()

Allsum <- bind_rows(k12sum, k12sumdistrict)
write.csv(Allsum, file=file.path('Wy_Ed_Jobs','allsum.csv'))

#Database for all data now by category
k12now <- k12jobs %>%
  filter(Archive_Date == max(Archive_Date))

k12nowsum <- k12now %>%
  group_by(Broad_Category) %>%
  summarize(
    Sum = n_distinct(title)
  ) %>%
  ungroup() %>%
  mutate(District = "Total")

k12districtnowsum <- k12now %>%
  group_by(Broad_Category, District) %>%
  summarize(
    Sum = n_distinct(title)
  ) %>%
  ungroup() %>%
  select(Broad_Category, Sum, District)

Allnow <- bind_rows(k12nowsum, k12districtnowsum)

write.csv(Allnow, file=file.path('Wy_Ed_Jobs','allnow.csv'))

list<-unique(k12jobs$District)
write.csv(list, "list.csv")
```


## Time to scrape Wyoming Higher Education Data
### This chunk scrapes LCCC's data
```{r Get Jobs from LCCC, message=FALSE, warning=FALSE, echo=TRUE}

library(rvest)
library(RSelenium)
library(dplyr)

# Start RSelenium with Firefox browser
rD <- rsDriver(browser = "firefox", port = 4444L, check = FALSE, chromever = NULL)
remDr <- rD$client

# Function to scrape job postings from a given page URL
scrape_jobs <- function(url) {
  remDr$navigate(url)
  
 # Scroll to bottom once
remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")

# Wait up to ~8 seconds for all job elements to appear
for(i in 1:16) {     # 16 × 0.5 sec = 8 sec total
  job_nodes <- remDr$findElements(using = "css selector",
                                  value = ".item-details-link")
  if(length(job_nodes) >= 30) break
  Sys.sleep(0.5)
}


  # Re-fetch the page after waiting for all jobs to load
  page_source <- remDr$getPageSource()[[1]]
  soup <- read_html(page_source)
  
  # Safely try to fetch job titles, links, locations, and posted dates
  job_titles <- tryCatch({
    soup %>% html_nodes(".item-details-link") %>% html_text(trim = TRUE)
  }, error = function(e) character(0))

  job_links <- tryCatch({
    soup %>% html_nodes(".item-details-link") %>% html_attr("href") %>%
      paste0("https://www.governmentjobs.com/careers/lcccwy?page=", .)
  }, error = function(e) character(0))

  # **Update to capture the Location** correctly for all jobs, including the last one
  job_location <- tryCatch({
    soup %>% html_nodes(".list-meta li:nth-child(1)") %>% html_text(trim = TRUE)
  }, error = function(e) character(0))
  
  
  posted_date <- tryCatch({
    soup %>% html_nodes(".list-entry-starts span") %>% html_text(trim = TRUE)
  }, error = function(e) character(0))

  # Ensure all vectors have the same length
  max_length <- max(length(job_titles), length(job_links), length(job_location), length(posted_date))
  
  job_titles <- c(job_titles, rep(NA, max_length - length(job_titles)))
  job_links <- c(job_links, rep(NA, max_length - length(job_links)))
  job_location <- c(job_location, rep(NA, max_length - length(job_location)))
  posted_date <- c(posted_date, rep(NA, max_length - length(posted_date)))

  # Create dataframe
  jobs_df <- data.frame(
    Title = job_titles,
    Location = job_location,
    Link = job_links,
    Posted_Date = posted_date,
    stringsAsFactors = FALSE
  )

  return(jobs_df)
}

# Scrape multiple pages
all_jobs <- list()

all_jobs <- list()

page <- 1
repeat {
  url <- paste0("https://www.governmentjobs.com/careers/lcccwy?page=", page)
  cat("Scraping:", url, "\n")

  jobs_df <- tryCatch({
    scrape_jobs(url)
  }, error = function(e) {
    cat("Error scraping page", page, ":", e$message, "\n")
    return(data.frame())
  })

  if (all(is.na(jobs_df$Title)) || nrow(jobs_df) == 0) {
    cat("No job titles found on page", page, ". Assuming end of listings.\n")
    break
  } else {
    all_jobs[[length(all_jobs) + 1]] <- jobs_df
    cat("Scraped page", page, "\n")
    page <- page + 1  # Don't forget to increment the page!
  }

  Sys.sleep(1)  # Be nice to the server
}

# Now outside the loop — combine and clean
lccc <- bind_rows(all_jobs) %>%
  mutate(Institution = "Laramie County Community College") %>%
  distinct(Title, .keep_all = TRUE)

# Close Selenium
remDr$close()
rD$server$stop()

# Cleanup and print results
lccc <- lccc %>%
  select(Title, Location, Posted_Date, Institution, Link)

print(lccc)

```

### This pulls data from Casper College
```{r casper college, message=FALSE, warning=FALSE, echo=TRUE}

# Define the URLs for page 1 and page 2
urls <- c(
  "https://www.schooljobs.com/careers/caspercollege", 
  "https://www.schooljobs.com/careers/caspercollege?page=2"
)

# Start RSelenium with Firefox browser
rD <- rsDriver(browser = "firefox", port = 4444L, check = FALSE, chromever = NULL)
remDr <- rD$client

# Initialize an empty data frame to store all job listings
casper <- data.frame()

# Loop over both pages to scrape data
for (url in urls) {
  
  remDr$navigate(url)
  Sys.sleep(10)  # Wait for dynamic content
  
  # Get page source and parse with rvest
  soup <- read_html(remDr$getPageSource()[[1]])
  
  # Extract job titles and links
  job_titles <- soup %>% 
    html_nodes(".item-details-link") %>% 
    html_text(trim = TRUE)
  
  job_links <- soup %>% 
    html_nodes(".item-details-link") %>% 
    html_attr("href") %>% 
    paste0("https://www.schooljobs.com/careers/caspercollege", .)
  
  # Extract department details
  departments <- soup %>% 
    html_nodes(".item-details-link") %>% 
    html_attr("data-department-name")
  
  if (length(departments) == 0) {
    departments <- rep(NA, length(job_titles))
  }
  
  # Extract job locations
  job_locations <- soup %>% 
    html_nodes(".list-meta li:nth-child(1)") %>% 
    html_text(trim = TRUE)
  
  if (length(job_locations) == 0) {
    job_locations <- rep(NA, length(job_titles))
  }
  
  # Extract posted date text (RELATIVE — keep as-is)
  Posted_Date <- soup %>% 
    html_nodes(".list-entry-starts span") %>% 
    html_text(trim = TRUE)
  
  # If no jobs found
  if (length(job_titles) == 0) {
    
    page_jobs <- data.frame(
      Title = "No Jobs",
      Location = NA,
      Posted_Date = NA,
      Institution = "Casper College",
      Link = url,
      stringsAsFactors = FALSE
    )
    
  } else {
    
    page_jobs <- data.frame(
      Title = job_titles,
      Location = job_locations,
      Posted_Date = Posted_Date,
      Institution = "Casper College",
      Link = job_links,
      stringsAsFactors = FALSE
    )
  }
  
  # Append results
  casper <- rbind(casper, page_jobs)
}

# Remove duplicates
casper <- unique(casper)

# Final column order & cleanup
casper <- casper %>%
  select(Title, Location, Posted_Date, Institution, Link) %>%
  filter(Title != "No Jobs")

# Print results
print(casper)

# Close RSelenium
remDr$close()
rD$server$stop()



```


### This pulls jobs data from Western Wyoming
```{r western wyoming, echo=TRUE, message=FALSE, warning=FALSE}
library(rvest)
library(RSelenium)
library(dplyr)
library(stringr)

#Convert date to days, weeks, more than 30 days
convert_date_to_relative <- function(date_chr) {
  date_parsed <- as.Date(date_chr, format = "%m/%d/%Y")
  
  if (is.na(date_parsed)) return(NA_character_)
  
  days_ago <- as.integer(Sys.Date() - date_parsed)
  
  if (days_ago < 1) {
    "Posted today"
  } else if (days_ago == 1) {
    "Posted 1 day ago"
  } else if (days_ago < 7) {
    paste("Posted", days_ago, "days ago")
  } else if (days_ago < 14) {
    "Posted 1 week ago"
  } else if (days_ago < 21) {
    "Posted 2 weeks ago"
  } else if (days_ago < 28) {
    "Posted 3 weeks ago"
  } else if (days_ago < 35) {
    "Posted 4 weeks ago"
  } else {
    "Posted more than 30 days ago"
  }
}


# Start RSelenium
rD <- rsDriver(browser = "firefox", port = 4444L, check = FALSE, chromever = NULL)
remDr <- rD$client

extract_jobs <- function(page_url) {
  remDr$navigate(page_url)
  Sys.sleep(5)  # allow full page load
  
  page_source <- remDr$getPageSource()[[1]]
  soup <- read_html(page_source)
  
  jobs <- soup %>% html_nodes("div.job-item.job-item-posting")
  if(length(jobs) == 0) return(NULL)
  
  job_list <- lapply(jobs, function(job) {
    # Title and Link
    title_node <- job %>% html_node("div.col-md-4.job-title h3 a")
    Title <- title_node %>% html_text(trim = TRUE)
    Link  <- paste0("https://wwccwy.peopleadmin.com", title_node %>% html_attr("href"))
    
    # Right column text (single string)
    right_text <- job %>% html_node("div.col-md-8") %>% html_text(trim = TRUE)
    
    # Posted_Date: first occurrence of MM/DD/YYYY
    raw_date <- str_extract(right_text, "\\d{2}/\\d{2}/\\d{4}")
    Posted_Date <- convert_date_to_relative(raw_date)

    
    # Location: last non-empty line that is NOT a date or code
    lines <- str_split(right_text, "\\n")[[1]] %>% str_trim()
    non_date <- lines[!str_detect(lines, "\\d{2}/\\d{2}/\\d{4}")]
    non_empty <- non_date[nzchar(non_date)]
    Location <- if(length(non_empty) > 0) tail(non_empty, 1) else NA
    
    data.frame(
      Title = Title,
      Location = Location,
      Posted_Date = Posted_Date,
      Link = Link,
      stringsAsFactors = FALSE
    )
  })
  
  do.call(rbind, job_list)
}

# Scrape pages 1 and 2
all_jobs <- do.call(rbind, lapply(1:2, function(p) {
  url <- paste0("https://wwccwy.peopleadmin.com/postings/search?page=", p)
  extract_jobs(url)
}))

# Add Institution column
western <- all_jobs %>% mutate(Institution = "Western Wyoming Community College")

# Save CSV
write.csv(western, "WWCC_job_listings.csv", row.names = FALSE)

# Print
print(western)

# Close RSelenium
remDr$close()
rD$server$stop()


```


### This pulls data from Central Wyoming college
```{r central wyoming, message=FALSE, echo=TRUE, warning=FALSE}
library(rvest)
library(RSelenium)
library(dplyr)
library(purrr)

# Start RSelenium with Firefox browser
rD <- rsDriver(browser = "firefox", port = 4444L, check = FALSE, chromever = NULL)
remDr <- rD$client

# Function to extract job postings from a page
extract_jobs <- function(page_url) {
  # Navigate to the page
  remDr$navigate(page_url)
  
  # Wait for dynamic content to load
  Sys.sleep(10)  # Adjust as needed based on page loading time
  
  # Get page source and parse with rvest
  page_source <- remDr$getPageSource()[[1]]
  soup <- read_html(page_source)
  
  # Extract job details
  job_titles <- soup %>% 
    html_nodes(".list-item .item-details-link") %>% 
    html_text(trim = TRUE)
  
  job_dates <- soup %>% 
    html_nodes(".list-published .list-entry-starts span") %>% 
    html_text(trim = TRUE)
  
  job_locations <- soup %>% 
    html_nodes(".list-item ul.list-meta li:nth-child(1)") %>% 
    html_text(trim = TRUE)
  
  # Combine data into a dataframe
  jobs_df <- data.frame(
    Title = job_titles,
    Location = job_locations,
    Posted_Date = job_dates,
    stringsAsFactors = FALSE
  )
  
  return(jobs_df)
}

# Initialize an empty data frame to store all jobs
all_jobs <- data.frame()

# Scrape both pages
for (page in 1:3) {
  page_url <- if (page == 1) {
    "https://www.schooljobs.com/careers/cwc"
  } else {
    paste0("https://www.schooljobs.com/careers/cwc?page= ", page)
  }
  
  page_jobs <- extract_jobs(page_url)
  
  if (nrow(page_jobs) > 0) {
    all_jobs <- bind_rows(all_jobs, page_jobs)
    cat("Scraped page", page, "\n")
  } else {
    cat("No data found on page", page, "\n")
  }
}

# Add the current date to the dataframe
central <- all_jobs %>%
  mutate(Institution = "Central Wyoming College")%>%
  mutate(Link = "https://www.schooljobs.com/careers/cwc")

# Print the dataframe
print(central)

# Close RSelenium
remDr$close()
rD$server$stop()


```

### This pulls jobs data from Eastern Wyoming college
```{r Eastern, message=FALSE, warning=FALSE, echo=TRUE}
library(rvest)
library(RSelenium)
library(dplyr)
library(stringr)
library(lubridate)
library(purrr)

convert_to_relative_date <- function(date_obj, reference_date = Sys.Date()) {
  diff_days <- as.numeric(difftime(reference_date, date_obj, units = "days"))
  
  if(is.na(diff_days)) return(NA_character_)
  else if(diff_days == 0) return("Today")
  else if(diff_days == 1) return("Yesterday")
  else if(diff_days < 7) return(paste(diff_days, "days ago"))
  else if(diff_days < 30) return(paste(round(diff_days/7), "weeks ago"))
  else return("More than 30 days ago")
}

# Start RSelenium
rD <- rsDriver(browser = "firefox", port = 4444L, check = FALSE, chromever = NULL)
remDr <- rD$client

extract_jobs <- function(page_url) {
  remDr$navigate(page_url)
  Sys.sleep(10)
  
  soup <- read_html(remDr$getPageSource()[[1]])
  
  job_nodes <- soup %>% html_nodes(".job-item.job-item-posting")
  if(length(job_nodes) == 0) return(NULL)
  
  job_list <- lapply(job_nodes, function(job) {
    title_node <- job %>% html_node(".job-title a")
    Title <- title_node %>% html_text(trim = TRUE)
    Link  <- paste0("https://ewc.peopleadmin.com", html_attr(title_node, "href"))
    
    # Navigate to individual job page to get Open Date
    remDr$navigate(Link)
    Sys.sleep(5)
    job_page <- read_html(remDr$getPageSource()[[1]])
    
    # Scrape Open Date from the <tr> where <th> = "Open Date"
    open_date_text <- job_page %>%
      html_nodes("tr") %>%
      keep(~ html_node(.x, "th") %>% html_text(trim = TRUE) == "Open Date") %>%
      html_node("td") %>%
      html_text(trim = TRUE)
    
    Posted_Date <- if(!is.na(open_date_text) && open_date_text != "") mdy(open_date_text) else NA
    
    data.frame(
      Title = Title,
      Link = Link,
      Posted_Date = Posted_Date,
      stringsAsFactors = FALSE
    )
  })
  
  do.call(rbind, job_list)
}

# Example: scrape first 2 pages
all_jobs <- do.call(rbind, lapply(1:2, function(p) {
  url <- paste0("https://ewc.peopleadmin.com/postings/search?page=", p)
  extract_jobs(url)
}))

# Add metadata and convert Posted_Date to relative
eastern <- all_jobs %>%
  mutate(
    Location = "Eastern Wyoming Campus",
    Institution = "Eastern Wyoming Community College",
    Posted_Date = map_chr(Posted_Date, convert_to_relative_date)
  ) %>%
  distinct()

print(eastern)

# Close RSelenium
remDr$close()
rD$server$stop()



```

### This pulls data Gillette college
```{r gillette, message=FALSE, echo=TRUE, warning=FALSE}
library(rvest)
library(RSelenium)
library(dplyr)

# Start RSelenium with Firefox browser
rD <- rsDriver(browser = "firefox", port = 4444L, check = FALSE, chromever = NULL)
remDr <- rD$client

# Function to extract job postings from a page
extract_jobs <- function(page_url) {
  # Navigate to the page
  remDr$navigate(page_url)
  
  # Wait for dynamic content to load
  Sys.sleep(10)  # Adjust as needed based on page loading time
  
  # Get page source and parse with rvest
  page_source <- remDr$getPageSource()[[1]]
  soup <- read_html(page_source)
  
  # Extract job details
  job_titles <- soup %>% 
    html_nodes(".list-item .item-details-link") %>% 
    html_text(trim = TRUE)
  
  job_dates <- soup %>% 
    html_nodes(".list-published .list-entry-starts span") %>% 
    html_text(trim = TRUE)
  
  job_locations <- soup %>% 
    html_nodes(".list-item ul.list-meta li:nth-child(1)") %>% 
    html_text(trim = TRUE)
  
  # Combine data into a dataframe
  jobs_df <- data.frame(
    Title = job_titles,
    Location = job_locations,
    Posted_Date = job_dates,
    stringsAsFactors = FALSE
  )
  
  return(jobs_df)
}

# Initialize an empty data frame to store all jobs
all_jobs <- data.frame()

# Scrape both pages
for (page in 1:5) {
  page_url <- if (page == 1) {
    "https://www.schooljobs.com/careers/gillettecollege"
  } else {
    paste0("https://www.schooljobs.com/careers/gillettecollege?page= ", page)
  }
  
  page_jobs <- extract_jobs(page_url)
  
  if (nrow(page_jobs) > 0) {
    all_jobs <- bind_rows(all_jobs, page_jobs)
    cat("Scraped page", page, "\n")
  } else {
    cat("No data found on page", page, "\n")
  }
}

# Add the current date to the dataframe
gillette <- all_jobs %>%
  mutate(Institution = "Gillette College")%>%
  mutate(Link = "https://www.schooljobs.com/careers/gillettecollege")

print(gillette)

# Close RSelenium
remDr$close()
rD$server$stop()

```

### This pulls data from Sheridan College
```{r sheridan, echo=TRUE, message=FALSE, warning=FALSE}
library(rvest)
library(RSelenium)
library(dplyr)
library(purrr)
library(stringr)
library(lubridate)

# Function to convert date to relative format
convert_to_relative_date <- function(date_obj, reference_date = Sys.Date()) {
  diff_days <- as.numeric(difftime(reference_date, date_obj, units = "days"))
  
  if(is.na(diff_days)) return(NA_character_)
  else if(diff_days == 0) return("Today")
  else if(diff_days == 1) return("Yesterday")
  else if(diff_days < 7) return(paste(diff_days, "days ago"))
  else if(diff_days < 30) return(paste(round(diff_days/7), "weeks ago"))
  else return("More than 30 days ago")
}

# Start RSelenium
rD <- rsDriver(browser = "firefox", port = 4444L, check = FALSE, chromever = NULL)
remDr <- rD$client

extract_jobs <- function(page_url) {
  remDr$navigate(page_url)
  Sys.sleep(10)  # allow full load
  
  soup <- read_html(remDr$getPageSource()[[1]])
  
  # Get job titles and links
  title_nodes <- soup %>% html_nodes(".col-md-4.job-title h3 a")
  Titles <- title_nodes %>% html_text(trim = TRUE)
  Links  <- title_nodes %>% html_attr("href") %>% paste0("https://jobs.sheridan.edu", .)
  
  if(length(Titles) == 0) return(NULL)
  
  job_list <- lapply(seq_along(Titles), function(i) {
    # Visit detail page for the posting
    remDr$navigate(Links[i])
    Sys.sleep(5)  # wait for detail page
    
    detail_page <- read_html(remDr$getPageSource()[[1]])
    
    # Extract "Job Posting Date"
    posted_date_text <- detail_page %>%
      html_nodes("th") %>%
      { .[str_detect(html_text(.), "Job Posting Date")] } %>%
      html_node(xpath = "following-sibling::td") %>%
      html_text(trim = TRUE)
    
    # Convert to Date
    Posted_Date <- if(length(posted_date_text) == 1) mdy(posted_date_text) else NA
    
    data.frame(
      Title = Titles[i],
      Link = Links[i],
      Posted_Date = Posted_Date,
      stringsAsFactors = FALSE
    )
  })
  
  do.call(rbind, job_list)
}

# Scrape first 3 pages (adjust if needed)
all_jobs_list <- lapply(1:3, function(p) {
  url <- paste0("https://jobs.sheridan.edu/postings/search?page=", p)
  extract_jobs(url)
})

# Remove NULL pages
all_jobs_list <- all_jobs_list[!sapply(all_jobs_list, is.null)]
all_jobs <- do.call(rbind, all_jobs_list)

# Add metadata and convert to relative date
sheridan <- all_jobs %>%
  mutate(
    Location = "Sheridan College Campus",
    Institution = "Sheridan College",
    Posted_Date = map_chr(Posted_Date, convert_to_relative_date)
  ) %>%
  distinct()

print(sheridan)

# Close RSelenium
remDr$close()
rD$server$stop()



```

### This pulls data from Northwest College
```{r Northwest, message=FALSE, warning=FALSE, echo=TRUE}
library(lubridate)
library(dplyr)
library(purrr)

convert_to_relative_date <- function(date_str, reference_date = Sys.Date()) {
  # Convert to Date
  actual_date <- mdy(date_str)  # assumes MM/DD/YYYY
  diff_days <- as.numeric(difftime(reference_date, actual_date, units = "days"))
  
  # Convert to relative format
  if(is.na(diff_days)) return(NA_character_)
  else if(diff_days == 0) return("Today")
  else if(diff_days == 1) return("Yesterday")
  else if(diff_days < 7) return(paste(diff_days, "days ago"))
  else if(diff_days < 30) return(paste(round(diff_days/7), "weeks ago"))
  else return("More than 30 days ago")
}

# Start RSelenium with Firefox browser
rD <- rsDriver(browser = "firefox", port = 4444L, check = FALSE, chromever = NULL)
remDr <- rD$client

# Function to extract job postings from a page
extract_jobs <- function(page_url) {
  # Navigate to the page
  remDr$navigate(page_url)
  
  # Wait for dynamic content to load
  Sys.sleep(10)  # Adjust as needed based on page loading time
  
  # Get page source and parse with rvest
  page_source <- remDr$getPageSource()[[1]]
  soup <- read_html(page_source)
  
  # Extract job titles, posted dates, and locations
  job_titles <- soup %>%
    html_nodes(".row.row-table .job-title h3 a") %>%
    html_text()
  
  posted_dates <- soup %>%
    html_nodes(".row.row-table .tbody-cell.col-3:nth-child(1)") %>%
    html_text()
  
  locations <- soup %>%
    html_nodes(".row.row-table .tbody-cell.col-3:nth-child(3)") %>%
    html_text()
  
  # Combine data into a dataframe
  jobs_df <- data.frame(
    Title = job_titles,
    Posted_Date = posted_dates,
    Location = locations,
    stringsAsFactors = FALSE
  )
  
  return(jobs_df)
}

# Initialize an empty data frame to store all jobs
all_jobs <- data.frame()

# Scrape the first 3 pages
for (page in 1:3) {
  page_url <- paste0("https://northwestcollege.simplehire.com/postings/search?page=", page)
  page_jobs <- extract_jobs(page_url)
  
  if (nrow(page_jobs) > 0) {
    all_jobs <- bind_rows(all_jobs, page_jobs)
    cat("Scraped page", page, "\n")
  } else {
    cat("No data found on page", page, "\n")
  }
}

northwest<- all_jobs%>%
  mutate(Institution = "Northwest College")%>%
  mutate(Link = "https://northwestcollege.simplehire.com/postings/search?page=")%>%
  select(Title, Location, Posted_Date, Institution, Link)

northwest <- northwest %>%
  mutate(
    Posted_Date = map_chr(Posted_Date, convert_to_relative_date)
  )

# Print the dataframe
print(northwest)

# Close RSelenium
remDr$close()
rD$server$stop()

```



### This pulls data from the University of Wyoming
```{r university of Wyoming}
library(RSelenium)
library(dplyr)
library(purrr)
library(lubridate)

# Function to convert dates to relative format
convert_to_relative_date <- function(date_str, reference_date = Sys.Date()) {
  if(is.na(date_str) || date_str == "") return(NA_character_)
  
  actual_date <- mdy(date_str)  # MM/DD/YYYY
  diff_days <- as.numeric(difftime(reference_date, actual_date, units = "days"))
  
  if(is.na(diff_days)) return(NA_character_)
  else if(diff_days == 0) return("Today")
  else if(diff_days == 1) return("Yesterday")
  else if(diff_days < 7) return(paste(diff_days, "days ago"))
  else if(diff_days < 30) return(paste(round(diff_days/7), "weeks ago"))
  else return("More than 30 days ago")
}

# Start RSelenium
rD <- rsDriver(browser = "firefox", port = 4444L, check = FALSE, chromever = NULL)
remDr <- rD$client

# Navigate to the job site
url <- "https://eeik.fa.us2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1/requisitions?"
remDr$navigate(url)
Sys.sleep(5)  # wait for initial load

# Scroll to load all jobs
scroll_pause_time <- 3
last_height <- remDr$executeScript("return document.body.scrollHeight")[[1]]

repeat {
  remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")
  Sys.sleep(scroll_pause_time)
  new_height <- remDr$executeScript("return document.body.scrollHeight")[[1]]
  if (new_height == last_height) break
  last_height <- new_height
}

# Extract job titles
titles_elems <- remDr$findElements(using = 'css selector', ".job-tile__title")
job_titles <- sapply(titles_elems, function(x) x$getElementText()) %>% unlist()

# Extract job locations
locations_elems <- remDr$findElements(using = 'css selector', "span[data-bind='html: primaryLocation']")
job_locations <- sapply(locations_elems, function(x) x$getElementText()) %>% unlist()

# Extract posted dates
dates_elems <- remDr$findElements(
  using = 'css selector',
  "div.job-list-item__job-info-label--posting-date + div div.job-list-item__job-info-value"
)
posted_dates <- sapply(dates_elems, function(x) x$getElementText()) %>% unlist()

# Check alignment
length(job_titles); length(job_locations); length(posted_dates)

# Combine into a dataframe
UW <- data.frame(
  Title = job_titles,
  Location = job_locations,
  Posted_Date = posted_dates,
  stringsAsFactors = FALSE
)

# Convert Posted_Date to relative format
UW <- UW %>%
  mutate(
    Institution = "University of Wyoming",
    Link = url,
    Posted_Date = map_chr(Posted_Date, convert_to_relative_date)
  ) %>%
  select(Title, Location, Posted_Date, Institution, Link)

# Print results
print(UW)

# Close RSelenium
remDr$close()
rD$server$stop()


```

### Combine all the Higher Education data
#### This will populate your app folder for your Shiny application
```{r munge he data, message=FALSE, warning=FALSE, echo=TRUE}
library(dplyr)
library(writexl)
library(tidyverse)
library(readxl)

# Combine all datasets
hedata <- rbind(lccc, casper, western, central, eastern, gillette, sheridan, northwest, UW)

hedata$Posted_Date<- as.character(hedata$Posted_Date)

hedata <- hedata %>%
  mutate(Archive_Date = Sys.Date())

# Save data with a time stamp
file_name <- paste0("hedata_", Sys.Date(), ".xlsx")

write_xlsx(hedata, path = file_name)

write_xlsx(hedata, path = file.path("Archived_HE_data", file_name))
write_xlsx(hedata, path = file.path("Wy_Ed_Jobs", "hedata.xlsx"))

csv_files <- list.files("Archived_HE_data", pattern = "\\.xlsx$", full.names = TRUE)

combined_HE_data <- csv_files %>%
  lapply(read_xlsx) %>%   
  bind_rows()

combined_HE_data$Archive_Date <- as.Date(combined_HE_data$Archive_Date)

combined_cat <- combined_HE_data %>%
  mutate(Job_Type = case_when(
    # Instructor/Teacher/Faculty
    grepl("Instructor|Instructional|Teacher|Faculty|Adjunct|Professor|Lecturer|Post Doc|Subject Matter Expert|Librarian|Educator", Title, ignore.case = TRUE) ~ "Instructor/Teacher/Faculty",
    
    # Student Positions
    grepl("Student Worker|Student Position|Work Study|Students Only|Student Library Aid|Student Employment", Title, ignore.case = TRUE) ~ "Student Positions",
    
    # Healthcare
    grepl("Nurse|Wellbeing|Health|Medicine|Medical Support Assistant", Title, ignore.case = TRUE) ~ "Healthcare",
    
    # Temporary Position
    grepl("Pooled Position|Temporary|Monthly Pooled", Title, ignore.case = TRUE) ~ "Temporary Position",
    
    # Staff (consolidated all office assistant/associate roles)
    grepl("Hourly|Administrative Assistant|Administrative Associate|Secretary|Library Assistant|Office Assistant|Office Associate", 
          Title, ignore.case = TRUE) ~ "Staff",
    
    # Advising
    grepl("Advisor|Advising|Enrollment Counselor|Financial Aid Counselor|Night ESL Class Aide|Part-time Academic Support Services Pool- Academic Support Tutor|Peer Tutor|Tribal Education Assistant", Title, ignore.case = TRUE) ~ "Advising and Student Services",
    
    # Professional
    grepl("Admissions|Coordinator|Registrar|Executive Assistant|Professional|Statistician|Proctor|Exec Assistant|Exec Assistant to President|Grant Writer|Graphic Designer|Negotiator|Botanist|College Relations|Controller|Database|Officer|Auditor|Web Site|Special Assistant|Retention Mentor|Buyer Assistant|Buyer, Textbooks|GEAR UP Event & College Coordination|Ombudsperson - Office of the President|Data Analyst|Employee Relations Specialist|Finance Technician|Grant Support Specialist|Marketing & Comm Spec.|Program Specialist", 
          Title, ignore.case = TRUE) ~ "Professional",
    
    # Administration
    grepl("\\bDean\\b|Vice President|Provost|Chief|Business Manager|Enrollment Specialist|Finance Technician|HR Records Specialist", Title, ignore.case = TRUE) ~ "Administration",
    
    # Coaching or Athletics
    grepl("coach|Athletic|Athletics", Title, ignore.case = TRUE) ~ "Coaching or Athletics",
    
    # Accounting/Finance
    grepl("Accountant|Receivable|Accounts", Title, ignore.case = TRUE) ~ "Accounting/Finance",
    
    # Research & Science (only actual research/scientific roles)
    grepl("Research Associate|Laboratory Technician|Research Scientist", Title, ignore.case = TRUE) ~ "Research & Science",
    
    # Technical
    grepl("Control Specialist|Engineer|IT Support Technician|Information Security Analyst|Programmer Analyst|Network Specialist|Specialist|Data Operation Engineer|Design Engineer|GIS Specialist", Title, ignore.case = TRUE) ~ "Technical",
    
    # Maintenance & Service
    grepl("Maintenance|Technician|Facilities|Grounds|HVAC|Plumber|Carpenter|Cook|Baker|Logistics Handler|Meat Plant Technician", Title, ignore.case = TRUE) ~ "Maintenance & Service",
    
    # Support Services
    grepl("Safety|Security|Custodian|Laborer|Driver|Cleaner|Facilities|Grounds|Library Information Specialist|Museum Exhibit Specialist|Specialist, Student Services|Campus Services I|Children's Center Part-Time Aide|KEY Camp Counselor|Lifeguard|Resident Assistant|Outdoor Recreation Lab Room|Part-Time Bookstore Sales Clerk|Physics Work-Study Assistant|Student Service Assistant|Subject Matter Expert, Life Enrichment Class", 
          Title, ignore.case = TRUE) ~ "Support Services",
    
    # Culinary
    grepl("Chef|Cook|Dining", Title, ignore.case = TRUE) ~ "Culinary",
    
    # Management
    grepl("Coordinator|Director|\\bDirector\\b|Manager|Supervisor|Assoc Director|Assoc Dir|Assist Dir|Asst Dir", 
          Title, ignore.case = TRUE) ~ "Management",
    
    TRUE ~ "Other"
  ))

facultydata <- combined_cat %>%
  filter(Job_Type == "Instructor/Teacher/Faculty") %>%
  mutate(Category = case_when(
    grepl("Law|Legal|College of Law", Title, ignore.case = TRUE) ~ "Legal",  # New category for law-related faculty
    
    grepl("\\bAgricultural Communications\\b|Agricultural", Title, ignore.case = TRUE) ~ "CTE",  # Keeping this up top
    
    grepl("Math", Title, ignore.case = TRUE) ~ "Math",
   
    grepl("Philosophy|English|Communication( Studies)?|Literature|Mass Communication", Title, ignore.case = TRUE) ~ "Humanities",  # More specific for Communication
    
    grepl("\\bComputer Science\\b|Computer|Zoology|Information Technology|Computing", Title, ignore.case = TRUE) ~ "CTE",
    
    grepl("Chemistry|Biology|Physics|Astronomy|Lab|Geology|Science|Artificial Intelligence", Title, ignore.case = TRUE) ~ "Science",
    
    grepl("History", Title, ignore.case = TRUE) ~ "History",
    
    grepl("Business|Accounting|Finance", Title, ignore.case = TRUE) ~ "CTE",
    
    grepl("Physical|Outdoor Activity|Athletic Trainer|Kinesiology|Strength", Title, ignore.case = TRUE) ~ "Physical Education",
    
    grepl("Health Technology|Nursing|Dental Hygiene|EMS|Medical|Medicine|Nutritional|Nutrition|Pharmacy", Title, ignore.case = TRUE) ~ "CTE",  # Removed generic "Health" to avoid misclassification
    
    grepl("Culinary|Hospitality|Tourism", Title, ignore.case = TRUE) ~ "Culinary/Hospitality",
    
    grepl("Technology|Equine|Industrial|Electrical|Instrumentation|CDL|Massage|Criminal Justice|Maintenance|Powerline|Substation|Construction|Diesel|Heavy Equipment|Supply Chain|Machine Tool|Welding|Mechanical|Engineering|Engineer|Energy|Printmaking", 
          Title, ignore.case = TRUE) ~ "CTE",
    
    grepl("Art|Graphic Design|Theater|Theatre|Acting|Music|Ceramics", Title, ignore.case = TRUE) ~ "The Arts",
    
    grepl("Criminal Justice", Title, ignore.case = TRUE) ~ "Criminal Justice",
    
    grepl("Human Services|Family", Title, ignore.case = TRUE) ~ "Human Services",
    
    grepl("Psychology|Anthropology|Sociology|Politics|Behavior|Social Work", Title, ignore.case = TRUE) ~ "Social Science",
    
    grepl("Spanish", Title, ignore.case = TRUE) ~ "Language",
    
    grepl("Education|Curriculum|Educational|Early Childhood", Title, ignore.case = TRUE) ~ "Education",
    
    grepl("library|librarian", Title, ignore.case = TRUE) ~ "Library",
    
    TRUE ~ "Uncategorized"
  ))
write.csv(facultydata, "facultydata.csv")
hesum <- facultydata %>%
  group_by(Category, Archive_Date) %>%
  summarize(
    sum = n()
  ) %>%
  ungroup()%>%
  mutate(Institution = "Total")%>%
  select(Category, Archive_Date, Institution, sum)

hesuminst<- facultydata%>%
  group_by(Category, Archive_Date, Institution)%>%
  summarise(
    sum = n()
  )%>%
  ungroup()

Allsum_he<-rbind(hesum, hesuminst)
write.csv(Allsum_he, file = file.path("Wy_Ed_Jobs", "allsum_he.csv"))

henow<- facultydata%>%
  filter(Archive_Date == max(Archive_Date))

henowsum <- henow %>%
  group_by(Category) %>%
  summarize(
    Sum = n()
  ) %>%
  ungroup()%>%
  mutate(Institution = "Total")

henowinstsum<- henow %>%
  group_by(Category, Institution)%>%
  summarize(
    Sum = n()
  ) %>%
  ungroup()%>%
  select(Category, Sum, Institution)

Allnow_he<- rbind(henowsum, henowinstsum)

write.csv(Allnow_he, file = file.path("Wy_Ed_Jobs", "allnow_he.csv"))
```



